Utilizing Every Image Object for Semi-supervised Phrase Grounding
Haidong Zhu Arka Sadhu Zhaoheng Zheng Ram Nevatia University of Southern California
{haidongz,asadhu,zhaoheng.zheng,nevatia}@usc.edu

arXiv:2011.02655v1 [cs.CV] 5 Nov 2020

Abstract
Phrase grounding models localize an object in the image given a referring expression. The annotated language queries available during training are limited, which also limits the variations of language combinations that a model can see during training. In this paper, we study the case applying objects without labeled queries for training the semisupervised phrase grounding. We propose to use learned location and subject embedding predictors (LSEP) to generate the corresponding language embeddings for objects lacking annotated queries in the training set. With the assistance of the detector, we also apply LSEP to train a grounding model on images without any annotation. We evaluate our method based on MAttNet on three public datasets: RefCOCO, RefCOCO+, and RefCOCOg. We show that our predictors allow the grounding system to learn from the objects without labeled queries and improve accuracy by 34.9% relatively with the detection results.
1. Introduction
The task of phrase grounding is to localize entities referred to by the given natural language phrases. This requires associating words and phrases from language modality to objects and relations in the visual modality. Grounding plays an important role in the applications for language and vision, such as visual question answering [2, 6, 9, 14, 38], image retrieval [31] and captioning [1, 30].
We extend grounder training to a semi-supervised setting, where we assume objects are only sparsely annotated with language. Typical phrase grounding datasets only use a subset of the objects in the image with densely query annotations for training. Modern grounding systems, such as MAttNet [35], are trained under fully supervised settings where every object used for training has a corresponding query. This limits the available data for training, where only images with dense annotations can be used. Fig. 1 shows two examples where some unlabeled objects cannot be used for training. Furthermore, the available queries limit the language variations that a model can see in the training set.

(a)

(b)

Figure 1. Images in the training set where only some objects (those

shown in red boxes) are labeled. Red boxes in the two images are

annotated as ‘black car’ and ‘white dog lying on the grass on the

left’ respectively, while objects in green boxes only have bounding

boxes and category names, ‘car’ and ‘dog’, associated with them.

We propose a language embedding prediction module for unlabeled objects using knowledge from other objects in the training set, allowing us to use every image object for training. Previous semi-supervised grounding systems [24, 18] still require full query annotations for the objects. To use the objects without labeled queries for training, simply using category names as the language queries is not effective; the queries in a grounding dataset are discriminative within a category, whereas the category names alone do not perform such discrimination. Our method can directly be trained on images without queries annotations by generating the corresponding language embeddings.
The embedding prediction module predicts query embeddings from visual information when the objects do not have associated phrases. In particular, we propose two embedding predictors to encode the subject and location properties from the given images. The predicted features are then combined with visual features to compute a grounding score. The predicted embeddings may not be perfect, but can still be useful in training. The predictors themselves are trained in an end-to-end framework; we call the resulting modules to be Location and Subject Embedding Predictors, abbreviated as LSEP. We emphasize that LSEP modules are used only in the grounder training phase; at inference time, we always have the needed language query.
To investigate the proposed semi-supervised setting, we use the following four-way characterization: (i) sparsely annotated across images, (ii) densely annotated fewer images,

1

(iii) only objects belong to a subset of the categories annotated, and (iv) only certain supercategories, the parent categories, of objects are annotated. To create these settings we subsample three commonly used datasets: RefCOCO [36], RefCOCO+ and RefCOCOg [20]. Using MAttNet as our base architecture, we observe consistent improvement by adding LSEP modules using the labeled bounding boxes, as well as on images without any annotation with a detector.
In summary, our contributions are three-fold: (i) we introduce a new semi-supervised setting for phrase grounding training with limited labeled queries, (ii) we propose subject and location embedding predictors for generating related language features from the visual information to narrow the gap between supervised and semi-supervised tasks, and (iii) we extend the training of a semi-supervised phrase grounding model to unlabeled images with a detector.
2. Related Work
Phrase Grounding has mainly been studied under the supervised setting, where query annotations are paired with the bounding boxes in the image. On popular phrase grounding datasets like RefCOCO [36, 20] and Flickr30k Entities [22, 34], state-of-the-art methods [35, 4, 12, 35, 15, 28] use a two-step process: ﬁnd all the objects using an object detector, such as Fast RCNN [8] or Mask RCNN [10], and then jointly reason over the query and the detected objects. QRC-Net [4] generates rewards for different proposals to match between visual and language modalities. MAttNet [35] introduces an automatic weight evaluation method for different components of the query to match with proposals. Some research [5, 15, 28, 29] try to apply attention [19] and visual-language transformer for cross-modality encoding between language queries and visual appearance. LXMert [29] applies the self-attention and cross-modality attention between the visual proposals and queries features to ﬁnd the corresponding. UNITER [5] uses the pre-training system and greatly improve the performance for different visual-language tasks. Recently, single-stage grounding methods [33, 25], where both steps are combined, have shown better performance on RefClef [13]. These models are pretrained on a large paired imagecaption data like conceptual captions [26] or aggregated vision+language datasets [29]. As a result, it is difﬁcult to evaluate them in the semi-supervised setting whose language annotations are scarce. In our work, we build on MattNet [35] to enable semi-supervised learning.
Semi-Supervised Phrase Grounding focuses on the grounding problem where language annotations are scarce, which has not been explored extensively. However, there are some closely related publications on this subject. Some researchers [24, 25, 3, 18] apply visual language consistency for ﬁnding the entities in an image when the bounding boxes for the proposals are not available. GroundeR

[24] considers semi-supervised localization where both language annotations and bounding boxes are present, while the association is provided only for a subset. Zero-Shot Grounding (ZSG) [25], in contrast, explores the grounding of novel objects for which bounding boxes are never seen. Some other methods, such as [32], use attention maps for ﬁnding the corresponding information without bounding boxes. These semi-supervised methods still require language annotations for proposals during training. Our proposed formulation assumes object bounding boxes are known, which can be easily acquired through the detection model, but language annotations for the detection boxes are not given. Compared with other existing semi-supervised grounding tasks, we can easily get the bounding boxes from a pretrained detector, while the missing of queries cannot be acquired without extra annotations.
3. Method
In this section, we introduce the phrase grounding task and MAttNet [35] in Sec. 3.1, followed by the detailed descriptions of LSEP and discussions in Sec. 3.2 and Sec. 3.3.
3.1. Background
A phrase grounding model takes a language query q and the objects in the image I as input. We use the enclosed bounding boxes to represent the objects. The bounding boxes are generated by a detector or come from groundtruth annotations, and the given query q is matched to a speciﬁc bounding box in the image. The grounding model calculates the score of every bounding box {oi}i=1,...,n and picks the box oq that best ﬁts the given query q. In the supervised regime [35, 24, 3, 5], every bounding box used for training is annotated with a speciﬁc query.
MAttNet splits a given query q into three separate parts, subject qsubj, location qloc and relationship qrel, by summing the one-hot vectors of corresponding words. MAttNet further predicts the relative weights, wsubj, wloc and wrel, for every part in the query, and extracts subject features vsubj,i, location features vloc,i and relationship features vrel,i from the bounding boxes {oi}i=1,...,n of the image I. The subject feature vsubj,i contains the visual appearance and category information of oi. The location feature vloc,i contains the absolute position of bounding box oi and the relative position from its N nearest neighbors. The relationship feature vrel,i includes both the relative position and visual representation of N nearest proposals around oi. Then it calculates the similarity S(oi|qsubj), S(oi|qloc) and S(oi|qrel) for every object-query pair. MAttNet calculates the ﬁnal score following
S(oi|r) = wsubj S(oi|qsubj ) +wlocS(oi|qloc) +wrelS(oi|qrel) (1)

Sub encoder

Subject feature

Loc encoder

Location feature

Rel encoder

Relation feature

Piece of pizza closest to brown plate at bottom

Bi-LSTM

'piece of pizza'

'at bottom'

Matching

Reweight
+
Matching

Overall Score

'closest to brown plate'

Matching

Sub encoder Loc encoder

Subject feature

Matching

Location feature

Reweight
+

Overall Score

< MISSING QUERY >

SEP

'blue chair'

LEP

'left side'

Matching

(a)

(b)

Figure 2. (a) MAttNet architecture [35] used to generate query embeddings when descriptive phrases are available (b) Subject and Location

Embedding predictors (SEP and LEP) which are used when only object category annotation is available.

During training, the model selects one positive pair (oi, qi) along with two negative pairs, (oi, qj) for negative query and (ok, qi) for negative proposal, following [35] to optimize L = Lrank + Lasuttbrj , where Lrank is
Lrank = i[λ1max(0, ∆ + S(oi, qj) − S(oi, qi)) +λ2max(0, ∆ + S(ok, qi) − S(oi, qi))] (2)
and Lasuttbrj is the cross-entropy loss for attribute prediction for oi. We follow [17] to select the attributes. When a bounding box is not labeled with a query, MAttNet treats it as a negative visual pair ok. During inference, the optimal bounding box oˆi is given by ﬁnding the maximum score in Eq. 1 among all the proposals to pair with the query q.
3.2. Model Framework
Using MAttNet [35] as our backbone network, we propose two embeddings prediction modules: a subject embedding predictor and a location embedding predictor to generate language embeddings when corresponding queries are missing. The complete LSEP framework is in Fig. 2.
Subject Embedding Predictor The Subject embedding predictor consists of an encoder that maps visual input for subject os to language dimension. We follow MAttNet [35] to extract the category feature vcat and attribute feature vattr. Then we concatenate these two features for use as the visual embedding for subject vsubj = (vattr; vcat). We use this module to transfer an existing attribute or embeddings for descriptive words to the known categories. We apply the following transformation
q˜subj = Wsubj (vattr; vcat) + bsubj
to generate the corresponding embedding q˜subj. Wsubj and bsubj are the weight and bias for the subject predictor. We use ”;” to represent the concatenation between two different features. During training, the grounding model takes q˜subj as language input when qsubj is missing. q˜subj lies in the same embedding space as qsubj since we use qsubj for supervision if it is available. The subject embedding predictor

transfers an embedding for attributes or descriptive words

to the object without the full query to complete its attribute.

Location Embedding Predictor To generate the cor-

responding language embedding, we extract the abso-

lute location of the bounding box as loca. Following [36, 37], we extract N relative location fea-

tures locr for the N -nearest bounding boxes around

it following [36, 37]. loca is 5-D vector loca in

the

form

of

[ xmin
w

,

ymin h

,

xmax w

,

ymax h

,

AreaBi AreaI

],

and

locr

is the concatenation of N vectors in the form of

[ ∆xmin
w

,

∆ymin h

,

∆xmax w

,

∆ymax h

,

AreaBi AreaBj

],

representing

the

relative position between N nearest objects and oi. w and

h represents the size of the image I; x and y represents the

properties of the bounding boxes. Location embedding pre-

dictor transfers the concatenation as loca and locr following

q˜loc = Wloc(loca; locr) + bloc

to the language embedding ql˜oc. Wloc and bloc are the weight and bias for the location module. By extracting visual embedding vloc following MAttNet [35], the grounding model takes the q˜loc as language input instead of qloc when
it is not available in the semi-supervised setting.

3.3. Discussion
In this subsection, we discuss some differences between LSEP and other existing methods, followed by the modiﬁed sampling policy, and why using the category names as queries doesn’t help for semi-supervised phrase grounding.
Differences with existing methods Existing semi and weakly supervised grounding methods [24, 32, 18] focus on the circumstances where the bounding boxes are not available. They pair an image directly with a query and use attention maps to ground objects, thus cannot use objects without descriptive queries during training. Since these methods cannot generate more variation of queries, these methods are still limited to the word combinations. In LSEP, we do not require query annotations for every bounding box. The

embedding predictors can generate the corresponding language embeddings from the visual embeddings.
Sampling policy For supervised grounding, all objects have descriptive queries associated with them. Thus, each object and query qualify as a positive or negative candidate. We apply a sampling strategy to accommodate the semisupervised setting. In this setting, part of bounding boxes have descriptive phrases associated with them as the supervised setting, while others only have a category name associated. For positive samples, both type of samples qualify, but LSEP can use the predicted language embedding as positive examples. For negative queries, we sample two negative pairs (oi, qj) and (ok, qi) for every positive pair (oi, qi). We sample the negative object ok and query qj from the available bounding boxes and queries, where the negative proposals and queries belong to the same object category as oi are preferred. When only the category name is available for qi or qj, we avoid pairing it with the object from the same category as negative pairs, e.g., the car enclosed by the red box in Fig. 1 (a) cannot be used as a negative example for the query ‘car’. In this case, proposals belong to different categories in the same image are preferred.
Usage Analysis When the bounding box only has the category name attached to it, an alternative to LSEP would be paring the bounding box with the query embedded generated by the category name. However, we ﬁnd that using such an embedding is not helpful, and the use of LSEP gives signiﬁcant improvements (as in Sec. 4.3). This improvement is due to additional discrimination provided by the two prediction modules. The descriptive queries are more effective at training a grounding system where the goal is to distinguish objects belonging to the same category.
Consider an example where a bounding box containing a dog whose color is brown and it is to the left of the image, but the only annotation is its category name (‘dog’). If an object is used as a positive example, the network must treat it the same as what the query describes, regardless of its location, color, or the similarities and differences of these entities compared with a negative query. Take the dog enclosed by the green bounding box in Fig. 1 (b) as an example. If we only use the category name ‘dog’ as its positive query and use ‘white dog lying on the grass on the left’ as the negative one, the grounding network cannot learn from the negative query that which part of the description is wrong, ‘white’, ‘lying on the grass’ or ‘on the left’. Predicting the subject and location embeddings provides a more accurate description for a more discriminative example. A similar analysis applies when we use the dog bounding box as a negative example; the network can better distinguish from the paired positive sample if the annotation is more speciﬁc. As each object box is selected as a positive sample once per epoch but not necessarily as a negative sample, the inﬂuence as a positive sample is much more critical.

4. Experiments
4.1. Datasets
We use three public datasets for evaluations: RefCOCO [36], RefCOCO+ and RefCOCOg [20], which are derived from MSCOCO [16] 2014 dataset. Queries in RefCOCO+ do not include absolute locations. MSCOCO [16] 2014 has 80 categories within 11 super categories. A supercategory is the parent category of categories that share the same properties. For example, both ‘bus’ and ‘car’ belong to the supercategory ‘vehicle’. We follow MAttNet [35] to create the training, validation and test sets. RefCOCO and RefCOCO+ include two test set split, testA and testB, where testA include the objects related with people, while testB include the objects that are irrelevant to people. RefCOCOg includes only one validation set and one test set. We compare our method with other methods on two tasks: supervised and semi-supervised phrase grounding. For the semisupervised task, we introduced four different data splits following i) annotation-based, ii) image-based, iii) categorybased, and iv) supercategory-based strategies.
Annotation-based selection is to randomly choose objects from all the annotations in the training set. For objects remaining, only bounding boxes equipped with category names are available during training.
Image-based selection is to select some images from the dataset and densely label the objects in these images with queries. For the objects in the remaining images, we only have corresponding bounding boxes and category names for the objects. Labeled queries for the same image will be used or discarded together.
Category-based selection is to select the phrases based on their categories. Half of the categories in the training set are annotated with their full queries, while only the category names along with the bounding boxes are available for the objects in the remaining categories.
Supercategory-based selection is to select the queries based on their parent category. We either use the labeled queries for the objects in the same supercategory, or replace them with their category names during training.
For category-based and supercategory-based settings, attributes other than the category names are not available during training if only category names are used. We select 40 categories and 6 supercategories and label them with full queries during training, and the attribute loss will not be calculated for the entities without full queries. These 6 supercategories include person, accessory, sports, kitchen, furniture and electronic, and the selected 40 categories are those whose category IDs are not divisible by 2. The inference is conducted on the whole set for annotation and image-based settings, while only on the remaining 40 categories and 5 supercategories which are not selected for the category and supercategory-based selection.

Grounding Accuracy
MAttNet LSEP
MAttNet LSEP
MAttNet LSEP
MAttNet LSEP

type
annotation annotation
image image
category category
supercategory supercategory

RefCOCO
Val testA testB
79.18 80.05 78.71 82.31 83.07 81.76
82.37 83.60 79.97 83.52 84.07 81.90
71.71 69.68 73.97 71.64 72.90 75.18
70.59 58.89 73.18 68.80 60.00 72.01

RefCOCO+
Val testA testB
55.73 61.07 49.27 61.11 65.33 53.21
68.05 70.41 61.77 68.83 71.77 64.10
50.51 44.00 49.68 51.73 47.33 52.91
48.54 39.23 50.27 48.61 38.67 50.16

RefCOCOg
Val test
71.74 70.77 72.80 73.97
74.75 73.44 75.87 75.92
59.82 60.34 63.90 63.41
56.88 54.31 56.54 53.93

F1 score
MAttNet LSEP
MAttNet LSEP
MAttNet LSEP
MAttNet LSEP

type
annotation annotation
image image
category category
supercategory supercategory

RefCOCO
Val testA testB
35.30 34.59 44.40 37.03 36.11 49.25
38.21 37.50 43.16 39.87 38.65 56.51
33.05 32.84 34.89 36.41 40.58 35.89
19.10 23.13 24.15 22.83 31.25 25.32

RefCOCO+
Val testA testB
27.89 27.04 40.97 29.14 28.27 42.61
29.17 28.16 44.40 30.02 29.07 45.83
21.25 15.38 22.17 24.79 17.39 26.14
16.06 8.33 19.41 17.64 11.67 20.51

RefCOCOg
Val test
41.35 40.03 43.10 41.44
43.23 42.75 45.11 43.51
25.91 23.44 32.86 32.08
10.08 12.77 25.21 26.37

Table 1. Accuracy and F1 score with groundtruth bounding boxes provided by the MSCOCO dataset. ‘Annotation’, ‘image’, ‘category’ and ‘supercategory’ represent annotation-based, image-based, category-based and supercategory-based selections respectively. The ratio of fully-labeled query is set to be 50% for all four settings.

4.2. Experimental Setup
In this subsection, we start with the details for the pipeline using for training and inference, followed by the implementation details and evaluation metrics.
Training and inference During the training period, our model face two different types of data: labeled objects, whose bounding boxes are labeled with groundtruth phrases, and unlabeled objects, whose bounding boxes are labeled with incomplete or no annotations. We train our model for 50000 iterations in all. We train the ﬁrst 20000 iterations on labeled objects. After 20000 iterations, we initialize the two predictors for 5000 iterations with the grounding model. In the remaining iterations, we i) train the grounding model on labeled objects; ii) train our predictors with the assistance of the grounding model on labeled objects, and iii) use the predictors to generate the language embeddings and apply them to train the grounding model on unlabeled ones. We do these three steps recurrently. The learning rate is 1e-4 and decays to half after every 8000 iterations. For the unlabeled objects, we set wsubj = wloc = 0.5 and wrel = 0 when applying ql˜oc and qs˜ubj as language embeddings. During the inference, we evaluate the score for the grounding model following Eq. 1 and use the bounding boxes with the highest conﬁ-

dence as our ﬁnal prediction for the given query. We follow MAttNet [35] use the bounding boxes from the groundtruth annotations of the MSCOCO dataset as candidate proposals for both training and inference for all methods.
Implementation Details For visual feature extraction, we follow [35] and apply a ResNet-101-based [11] Faster RCNN [8] to extract vsubj and vrel for its appearing. We ﬁrst encode the original query word by word into one-hot vectors for language features, then extract the feature with a bi-LSTM. We extract the visual feature from the C3 and C4 layers from the same ResNet to build the subject embedding predictor. The subject feature is the concatenation of C3 and C4 features after two 1x1 convolutional kernels that do not share weights. The category feature is the C4 feature, followed by a 1x1 convolutional kernel. The visual embeddings for subjects and locations are sent into the subject and location embedding predictors respectively, which are both 2-layer MLPs with a 512-dim hidden layer. The activation function is set as ReLU for the pipeline.
Metrics We apply two metrics for evaluation: Accuracy for phrase grounding and F1 score for attribute prediction. The accuracy for phrase grounding is calculated as the percentage of correct bounding box predictions compared with the number of queries. The F1 score is the harmonic mean

of the precision and recall for the attribute prediction. We follow [13] to parse the queries into 7 parts: category name, color, size, absolute location, relative location, relative object, and generic attribute. We evaluate the F1 score on the three types of attributes handled by the subject embedding predictor: category name, color, and generic attributes.
4.3. Results and Analysis
In this subsection, we ﬁrst show quantitative results for four semi-supervised settings with different ratios of annotations, followed by the results using a detector instead of the groundtruth bounding boxes and some ablations about how much every module in LSEP contributes.
Four-way characterization. We show results for four different selection settings in Table 1. For every case, 50% of the objects have language queries, while the remaining 50% only have category names. As shown in Table 1, adding LSEP module to MAttNet improves accuracy for each of the annotation-based, image-based, and categorybased selection settings. However, for the supercategorybased setting, it remains the same. Comparing the results between category-based and supercategory-based selection settings, we show that the grounding model can apply features from a nearby category, even labels for such categories are not available, since the objects in the same supercategory share similar visual features.
Interestingly, we ﬁnd that F1 scores for all four settings signiﬁcantly beneﬁt from LSEP, indicating that the ﬁnal attribute accuracy improves with our subject embeddings predictor. Better F1 results for all four selection settings show that LSEP enables better transfers of attribute combinations compared with MAttNet.
Different ratios of available queries. We analyze the performance on different ratios of annotated queries in Table 2. We use 25%, 50%, 75%, 100% of the queries for annotation and image-based selection settings, where 100%, i.e., every bounding box used for training is paired with a query, refers to the fully-supervised case. In this case, we ﬁrst train the two predictors with groundtruth annotations. We then use these two language encoders to generate corresponding embeddings from the encoded visual embeddings and apply them for training the grounding model. We make the following observations.
(i) Fully-supervised Results. LSEP gives mild improvements over Mattnet when 100% of the query annotations are available. Compared with the fully supervised method, LSEP can extract further information besides information in the labeled queries.
(ii) Different amount of annotations. With fewer available queries in the training set, the performances for both MAttNet and LSEP go down, while LSEP shows consistent improvement compared with MAttNet on

both annotation-based selection and image-based selection settings. LSEP can narrow the gap by around 40% between the results of MAttNet to their supervised performance.
(iii) Annotation density in the image. Results for the image-based setting shows higher phrase grounding accuracy than the annotation-based selection setting with similar available queries, indicating that denselyannotated images help a grounding model ﬁnd the differences between objects. Objects in the same image create more challenging object-query pairs for the phrase grounding model to distinguish the subtle differences in the same surroundings and circumstances.
(iv) On use of category names. Comparing the results of using the category names as the full queries during training, we notice that both MAttNet and LSEP show similar performance when not using them. When using the category name as the full query, positive and negative examples can not be from the same category. Distinguishing two objects in the same category is a more challenging task and helps the network ﬁnd more useful descriptive information than ﬁnding the differences between two objects from different classes.
Results of Using Detection Outputs Instead of using MSCOCO groundtruth boxes, we compare the semisupervised phrase grounding results with the proposals generated by a pretrained detector. We use the image-based selection as the experiment setting, and set the percentage of images that have been annotated as 50% for all conﬁgurations. We use a Faster R-CNN [23] trained on MSCOCO 2014 [16] detection task provided by MAttNet [35] as our detector. During training, we use the proposal with the maximum IoU with the groundtruth bounding box for those objects with labeled queries. For the remaining 50% that are unlabeled with any query, we use the detection results and their category names provided by the detector. For inference, we select among the detected bounding boxes generated by the Faster R-CNN.
We show the grounding accuracy on all three datasets in Table 3 with the fully supervised results for comparison, where all queries are available. By comparing the gap between supervised results of MAttNet and the settings when the number of labeled bounding boxes is limited, we ﬁnd that the grounding accuracy is relatively 34.9% better on average and LSEP shows consistent improvement for all three datasets when applying it for predicting the query embeddings, indicating that we can apply LSEP for extracting information to learn from the images without any annotations in addition to the labeled proposals.
Contribution of Location and Subject Modules We conduct an ablation study on how much the two embedding predictors contribute to the improvement of

Accu-Att [7] PLAN [39] Multi-hop [27] NegBag [21] S-L-R [37] MAttNet [35] LSEP
MAttNet w/o cat. LSEP w/o cat. MAttNet LSEP
MAttNet w/o cat. LSEP w/o cat. MAttNet LSEP
MAttNet w/o cat. LSEP w/o cat. MAttNet LSEP
MAttNet w/o cat. LSEP w/o cat. MAttNet LSEP
MAttNet w/o cat. LSEP w/o cat. MAttNet LSEP
MAttNet w/o cat. LSEP w/o cat. MAttNet LSEP

Type /
Labeled %
100% 100% 100% 100% 100% 100% 100%
annotation-75% annotation-75% annotation-75% annotation-75%
annotation-50% annotation-50% annotation-50% annotation-50%
annotation-25% annotation-25% annotation-25% annotation-25%
image-75% image-75% image-75% image-75%
image-50% image-50% image-50% image-50%
image-25% image-25% image-25% image-25%

RefCOCO
Val testA testB
81.27 81.17 80.01 81.67 80.81 81.32 84.90 87.40 83.10 76.90 75.60 78.00 79.56 78.95 80.22 85.65 85.26 84.57 85.71 85.69 84.26
81.78 83.54 79.26 83.02 84.70 79.60 81.89 83.52 79.48 83.11 84.46 79.58
79.33 80.07 78.05 82.11 83.60 80.96 79.18 80.05 78.71 82.31 83.07 81.76
63.73 66.01 62.88 67.43 70.55 66.01 63.02 65.59 62.93 67.74 70.51 66.01
82.89 83.45 81.51 83.70 84.38 82.85 82.87 84.01 81.39 83.91 84.16 82.87
82.40 83.33 80.79 83.34 83.54 82.16 82.37 83.60 79.97 83.52 84.07 81.90
79.92 80.25 78.81 82.32 82.77 80.48 79.81 80.59 78.76 82.75 82.53 81.02

RefCOCO+
Val testA testB
65.56 68.76 60.63 64.18 66.31 61.46 73.80 78.70 65.80
--62.26 64.60 59.62 71.01 75.13 66.17 71.99 75.36 66.25
61.08 64.83 56.58 67.53 70.07 61.51 61.72 64.87 56.53 68.01 70.47 61.49
55.59 60.95 49.46 60.28 65.19 53.18 55.73 61.07 49.27 61.07 65.33 53.21
42.96 48.10 39.76 46.94 52.81 41.42 42.99 48.12 39.75 46.96 53.01 41.85
68.40 71.18 64.14 70.12 73.05 64.59 68.47 71.09 64.18 70.09 72.98 64.61
67.42 70.19 61.96 68.85 71.08 63.31 68.05 70.41 61.77 68.83 71.77 64.10
63.90 66.92 59.85 67.22 69.80 62.38 63.95 66.92 59.81 67.19 69.97 62.53

RefCOCOg
Val testA
---- 68.40 71.65 71.92 78.10 78.12 78.96 78.29
72.14 72.07 75.53 74.82 72.30 72.02 75.62 74.89
71.20 70.75 73.71 72.93 71.74 70.77 72.80 73.97
67.10 67.13 69.08 69.52 67.21 67.09 69.06 69.58
75.76 75.32 75.94 76.15 75.99 75.26 76.27 76.09
74.61 73.65 75.94 75.35 74.75 73.44 75.87 75.92
68.34 68.20 73.14 72.42 68.40 68.07 73.25 72.08

Table 2. Semi-supervised results for grounding for annotation-based and image-based selections. ‘Annotation’ and ‘image’ represent annotation-based and image-based selections, and the number after dash represent the percentage of bounding boxes that has been labeled with queries during training. Methods ending with “w/o cat.” do not use category names as labeled queries.

semi-supervised grounding accuracy. We use the RefCOCO dataset with 50% annotations available based on annotation-based selection as our setting. We show the results in Table 4. We observe that both the subject and location embedding predictors improve the grounding accuracy compared with the model without any embedding predictor. The combination of two predictors has the highest score. Both subject and object predictors help with better grounding results compared with original MAttNet.
4.4. Qualitative Results
We show some visualization results in Fig. 3. Results in the four rows are for annotation-based, image-based, category-based, and supercategory-based selection settings respectively. We show the grounding results for MAttNet

of the same pair on the left and LSEP on the right. We calculate the results for the category-based and supercategorybased selection settings on the categories whose full queries are not available during training. We ﬁnd that MAttNet successfully ﬁnds the object of the same category, such as the ‘bottle’ in the ﬁrst example of the second row, but it fails to ﬁnd the one based on the given query, while LSEP successfully localizes the third bottle from the left in the image.
5. Conclusion
We study the task of using unlabeled object data to improve the accuracy of a phrase grounding system by using embedding predictor modules. This approach also allows us to introduce new categories, with available detectors, in

(a) The donut in the front

(b) A white and blue bag on top of a black suitcase

(a) Third bottle from the left

(b) Second stack of <UNK> from right

(c) The right half of the left sandwich

(d) Chair her foot is on

(c) The giraffe whose head is not visible

(d) Right side second banana up

Figure 3. Visualization results. The images on the left are the results for MAttNet and images on the right are the results for LSEP.

Dataset RefCOCO
RefCOCO+
RefCOCOg

Split
val testA testB
val testA testB
val test

MAttNet (FS)
75.78 82.01 70.03
65.88 72.02 57.03
66.87 67.03

MAttNet
73.17 79.54 67.83
63.95 69.33 53.60
63.29 62.97

LSEP
74.25 80.47 68.59
64.67 70.25 55.01
64.28 64.01

Table 3. Phrase grounding ccuracy with Fast RCNN detector on image-based selection with 50% fully annotated queries. MAttNet (FS) refers to the fully supervised result where 100% labeled queries are available.

phrase grounding. We show the improvements in accuracy by using subject and location embedding predictors applied to MAttNet [35] for semi-supervised grounding tasks.

Method MAttNet

val testA testB

79.18 80.05 78.71

SEP-Net
80.85 82.09 79.57

LEP-Net
80.98 81.81 78.88

LSEP
82.31 83.07 81.76

Table 4. Ablation study. LEP-Net only uses the location embedding predictor and SEP-Net uses the subject embedding predictor.

Acknowledgments
This work was supported by the U.S. DARPA AIDA Program No. FA8750-18-2-0014. The views and conclusions contained in this document are those of the authors and should not be interpreted as representing the ofﬁcial policies, either expressed or implied, of the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for Government purposes notwithstanding any copyright notation here on.

References
[1] Peter Anderson, Xiaodong He, Chris Buehler, Damien Teney, Mark Johnson, Stephen Gould, and Lei Zhang. Bottom-up and top-down attention for image captioning and visual question answering. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 6077–6086, 2018.
[2] Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C Lawrence Zitnick, and Devi Parikh. Vqa: Visual question answering. In Proceedings of the IEEE international conference on computer vision, pages 2425– 2433, 2015.
[3] Kan Chen, Jiyang Gao, and Ram Nevatia. Knowledge aided consistency for weakly supervised phrase grounding. In CVPR, 2018.
[4] Kan Chen, Rama Kovvuri, and Ramakant Nevatia. Queryguided regression network with context policy for phrase grounding. 2017 IEEE International Conference on Computer Vision (ICCV), pages 824–832, 2017.
[5] Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, and Jingjing Liu. Uniter: Learning universal image-text representations. arXiv preprint arXiv:1909.11740, 2019.
[6] Abhishek Das, Samyak Datta, Georgia Gkioxari, Stefan Lee, Devi Parikh, and Dhruv Batra. Embodied question answering. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops, pages 2054–2063, 2018.
[7] Chaorui Deng, Qi Wu, Qingyao Wu, Fuyuan Hu, Fan Lyu, and Mingkui Tan. Visual grounding via accumulated attention. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 7746–7755, 2018.
[8] Ross Girshick. Fast r-cnn. In Proceedings of the IEEE international conference on computer vision, pages 1440–1448, 2015.
[9] Daniel Gordon, Aniruddha Kembhavi, Mohammad Rastegari, Joseph Redmon, Dieter Fox, and Ali Farhadi. Iqa: Visual question answering in interactive environments. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4089–4098, 2018.
[10] Kaiming He, Georgia Gkioxari, Piotr Dolla´r, and Ross Girshick. Mask r-cnn. In Proceedings of the IEEE international conference on computer vision, pages 2961–2969, 2017.
[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770–778, 2016.
[12] Ronghang Hu, Marcus Rohrbach, Jacob Andreas, Trevor Darrell, and Kate Saenko. Modeling relationships in referential expressions with compositional modular networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 1115–1124, 2017.
[13] Sahar Kazemzadeh, Vicente Ordonez, Mark Matten, and Tamara Berg. Referitgame: Referring to objects in photographs of natural scenes. In Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), pages 787–798, 2014.

[14] Liang Li, Shuhui Wang, Shuqiang Jiang, and Qingming Huang. Attentive recurrent neural network for weaksupervised multi-label image classiﬁcation. In Proceedings of the 26th ACM international conference on Multimedia, pages 1092–1100, 2018.
[15] Liunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh, and Kai-Wei Chang. Visualbert: A simple and performant baseline for vision and language. arXiv preprint arXiv:1908.03557, 2019.
[16] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dolla´r, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In European conference on computer vision, pages 740–755. Springer, 2014.
[17] Jingyu Liu, Liang Wang, and Ming-Hsuan Yang. Referring expression generation and comprehension via attributes. In Proceedings of the IEEE International Conference on Computer Vision, pages 4856–4864, 2017.
[18] Xuejing Liu, Liang Li, Shuhui Wang, Zheng-Jun Zha, Li Su, and Qingming Huang. Knowledge-guided pairwise reconstruction network for weakly supervised referring expression grounding. In Proceedings of the 27th ACM International Conference on Multimedia, pages 539–547, 2019.
[19] Xihui Liu, Zihao Wang, Jing Shao, Xiaogang Wang, and Hongsheng Li. Improving referring expression grounding with cross-modal attention-guided erasing. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 1950–1959, 2019.
[20] Junhua Mao, Jonathan Huang, Alexander Toshev, Oana Camburu, Alan L Yuille, and Kevin Murphy. Generation and comprehension of unambiguous object descriptions. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 11–20, 2016.
[21] Varun K Nagaraja, Vlad I Morariu, and Larry S Davis. Modeling context between objects for referring expression understanding. In European Conference on Computer Vision, pages 792–807. Springer, 2016.
[22] Bryan A. Plummer, Liwei Wang, Christopher M. Cervantes, Juan C. Caicedo, Julia Hockenmaier, and Svetlana Lazebnik. Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence models. IJCV, 123(1):74–93, 2017.
[23] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object detection with region proposal networks. In Advances in neural information processing systems, pages 91–99, 2015.
[24] Anna Rohrbach, Marcus Rohrbach, Ronghang Hu, Trevor Darrell, and Bernt Schiele. Grounding of textual phrases in images by reconstruction. In European Conference on Computer Vision, pages 817–834. Springer, 2016.
[25] Arka Sadhu, Kan Chen, and Ram Nevatia. Zero-shot grounding of objects from natural language queries. In Proceedings of the IEEE International Conference on Computer Vision, pages 4694–4703, 2019.
[26] Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning. In ACL, 2018.

[27] Florian Strub, Mathieu Seurin, Ethan Perez, Harm de Vries, Je´re´mie Mary, Philippe Preux, and Aaron CourvilleOlivier Pietquin. Visual reasoning with multi-hop feature modulation. In Proceedings of the European Conference on Computer Vision (ECCV), pages 784–800, 2018.
[28] Weijie Su, Xizhou Zhu, Yue Cao, Bin Li, Lewei Lu, Furu Wei, and Jifeng Dai. Vl-bert: Pre-training of generic visuallinguistic representations. arXiv preprint arXiv:1908.08530, 2019.
[29] Hao Tan and Mohit Bansal. Lxmert: Learning crossmodality encoder representations from transformers. arXiv preprint arXiv:1908.07490, 2019.
[30] Oriol Vinyals, Alexander Toshev, Samy Bengio, and Dumitru Erhan. Show and tell: A neural image caption generator. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 3156–3164, 2015.
[31] Shuhui Wang, Yangyu Chen, Junbao Zhuo, Qingming Huang, and Qi Tian. Joint global and co-attentive representation learning for image-sentence retrieval. In Proceedings of the 26th ACM international conference on Multimedia, pages 1398–1406, 2018.
[32] Fanyi Xiao, Leonid Sigal, and Yong Jae Lee. Weaklysupervised visual grounding of phrases with linguistic structures. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 5945–5954, 2017.
[33] Zhengyuan Yang, Boqing Gong, L. Wang, Wenbing Huang, Dong Yu, and Jiebo Luo. A fast and accurate one-stage approach to visual grounding. 2019 IEEE/CVF International Conference on Computer Vision (ICCV), pages 4682–4692, 2019.
[34] Peter Young, Alice Lai, Micah Hodosh, and Julia Hockenmaier. From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions. TACL, 2:67–78, 2014.
[35] Licheng Yu, Zhe Lin, Xiaohui Shen, Jimei Yang, Xin Lu, Mohit Bansal, and Tamara L Berg. Mattnet: Modular attention network for referring expression comprehension. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 1307–1315, 2018.
[36] Licheng Yu, Patrick Poirson, Shan Yang, Alexander C Berg, and Tamara L Berg. Modeling context in referring expressions. In European Conference on Computer Vision, pages 69–85. Springer, 2016.
[37] Licheng Yu, Hao Tan, Mohit Bansal, and Tamara L Berg. A joint speaker-listener-reinforcer model for referring expressions. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 7282–7290, 2017.
[38] Yundong Zhang, Juan Carlos Niebles, and Alvaro Soto. Interpretable visual question answering by visual grounding from attention supervision mining. In 2019 IEEE Winter Conference on Applications of Computer Vision (WACV), pages 349–357. IEEE, 2019.
[39] Bohan Zhuang, Qi Wu, Chunhua Shen, Ian Reid, and Anton van den Hengel. Parallel attention: A uniﬁed framework for visual object discovery through dialogs and queries. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4252–4261, 2018.

