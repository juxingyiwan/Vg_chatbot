Title:          M6: Multi-Modality-to-Multi-Modality Multitask Mega-transformer for Unified Pretraining
Subject:        -  Computing methodologies  ->  Natural language processing.Computer vision.
Keywords:       Multi-modal pretraining; Large-scale pretraining; Cross-modal understanding and generation
Author:          Junyang Lin1*, Rui Men1*, An Yang1*, Chang Zhou1, Yichang Zhang1, Peng Wang1, Jingren Zhou1, Jie Tang2, Hongxia Yang1 
Creator:        LaTeX with acmart 2020/01/11 v1.67 Typesetting articles for the Association for Computing Machinery and hyperref 2020-05-15 v7.00e Hypertext links for LaTeX
Producer:       xdvipdfmx (20200315); modified using iText 4.2.0 by 1T3XT
CreationDate:   06/04/21 10:19:08
ModDate:        12/31/23 04:45:36
Tagged:         no
Form:           AcroForm
Pages:          11
Encrypted:      no
Page size:      612 x 792 pts (letter) (rotated 0 degrees)
File size:      3353261 bytes
Optimized:      no
PDF version:    1.5
